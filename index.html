<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Audio samples from "VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design"</title>
    <meta charset="UTF-8">
    <title>VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design</title>
    <style>
        red {color: red}
        audio {width: 250px}
    </style>
  </head>
  <body>
    <article>
      <header>
        <h1>Audio samples from "VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design"</h1>
      </header>
    </article>

<!--    <p><b>Paper:</b> <a href="">arXiv</a></p>-->
<!--    <p><b>Authors</b>: </p>-->

    <div>
      <b>Abstract:</b>
      Recently, single-stage text-to-speech models have been actively studied,
      and the works have shown better quality than two-stage pipeline systems.
      In this paper, we describe VITS2, which efficiently synthesizes more natural speech by improving several aspects of the previous,
      widely referenced single-stage text-to-speech work.
      We improve quality and efficiency by designing a duration predictor with adversarial learning
      and propose structural improvements to increase the performance of the augmented variational autoencoder.
      Further, We propose a method to improve speaker similarity in the multi-speaker model compared to the previous work.
      Our method shows a higher mean opinion score than the previous work,
      and it is also superior in evaluating the similarity of speech characteristics in the multi-speaker model.
      Moreover, the proposed method shows higher efficiency than the previous work in training as well as in synthesis,
      which we confirm by comparing its computation speed with that of the previous work.
    <p></p>
    </div>

    <p class="toc_title">Contents</p>
    <div id="toc_container">
    <ul>
      <li><a href="#ss">Single Speaker (LJ Speech Dataset)</a></li>
      <li><a href="#ms">Multiple Speakers (VCTK Dataset)</a></li>
    </ul>
    </div>
  </body>
</html>